{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c595de0c",
   "metadata": {},
   "source": [
    "EXTRACTING CSV AND PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d764e938",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pdfplumber in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (0.10.2)\n",
      "Requirement already satisfied: Pillow>=9.1 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from pdfplumber) (9.4.0)\n",
      "Requirement already satisfied: pypdfium2>=4.18.0 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from pdfplumber) (4.18.0)\n",
      "Requirement already satisfied: pdfminer.six==20221105 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from pdfplumber) (20221105)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from pdfminer.six==20221105->pdfplumber) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from pdfminer.six==20221105->pdfplumber) (39.0.1)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (1.15.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20221105->pdfplumber) (2.21)\n",
      "Requirement already satisfied: fpdf in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (1.7.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install pdfplumber\n",
    "!pip install fpdf\n",
    "import pdfplumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dfe7034f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables extracted and saved as CSV files.\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "csv_files = []  # List to store paths of created CSV files\n",
    "\n",
    "def is_empty_table(table):\n",
    "    \"\"\"Check if the table is empty or has no content.\"\"\"\n",
    "    for row in table:\n",
    "        if any(cell for cell in row if cell and cell.strip()):  # Check if any cell has content\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# Specify the directory containing the PDF files\n",
    "directory = \"data\"\n",
    "csv_storage = \"csv_storage\"\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if the file is a PDF\n",
    "    if filename.endswith(\".pdf\"):\n",
    "        filepath = os.path.join(directory, filename)\n",
    "        \n",
    "        with pdfplumber.open(filepath) as pdf:\n",
    "            for page_number, page in enumerate(pdf.pages):\n",
    "                table = page.extract_table()\n",
    "                \n",
    "                if table and not is_empty_table(table):  # Check if table is not None and not empty\n",
    "                    # Convert the table to a DataFrame\n",
    "                    df = pd.DataFrame(table[1:], columns=table[0])\n",
    "                    \n",
    "                    # Create a CSV filename based on the PDF filename and page number\n",
    "                    csv_filename = filename.split('.')[0] + f'_page{page_number}.csv'\n",
    "                    csv_filepath = os.path.join(csv_storage, csv_filename)\n",
    "                    \n",
    "                    # Save the DataFrame to a CSV file\n",
    "                    df.to_csv(csv_filepath, encoding='utf-8-sig', index=False)\n",
    "                    \n",
    "                    # Append the path of the created CSV file to the list\n",
    "                    csv_files.append(csv_filepath)\n",
    "\n",
    "print(\"Tables extracted and saved as CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb0e7475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    }
   ],
   "source": [
    "total_pages = len(pdf.pages)\n",
    "print(total_pages)\n",
    "# with pdf as f:\n",
    "#   for i in f.pages:\n",
    "#     txt += i.extract_tables()\n",
    "# page = pdf.pages[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ba4fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pdfplumber.open(filepath) as pdf:\n",
    "    txt = ''\n",
    "    for page in pdf.pages:\n",
    "        # Extract text from the page\n",
    "        txt += page.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "71e26cab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text extracted and saved as PDF files.\n"
     ]
    }
   ],
   "source": [
    "from fpdf import FPDF\n",
    "\n",
    "def add_wrapped_text(pdf, text, max_width, line_height):\n",
    "    words = text.split(' ')\n",
    "    lines = []\n",
    "    current_line = \"\"\n",
    "    \n",
    "    for word in words:\n",
    "        if current_line:  # If there's already content in current_line, add a space before the next word\n",
    "            test_line = current_line + \" \" + word\n",
    "        else:\n",
    "            test_line = word\n",
    "        \n",
    "        test_width = pdf.get_string_width(test_line)\n",
    "        \n",
    "        if test_width <= max_width:\n",
    "            current_line = test_line\n",
    "        else:\n",
    "            lines.append(current_line)\n",
    "            current_line = word  # Start the new line with the current word without adding a space before it\n",
    "\n",
    "    if current_line:  # Append any remaining content to lines\n",
    "        lines.append(current_line)\n",
    "\n",
    "    for line in lines:\n",
    "        line_encoded = line.encode('latin-1', 'replace').decode('latin-1')\n",
    "        pdf.cell(max_width, line_height, line_encoded)\n",
    "        pdf.ln(line_height)\n",
    "\n",
    "text = txt  # Insert the text you want to fit\n",
    "font_size = 12\n",
    "line_height = font_size * 1.5  # 1.5 line spacing\n",
    "\n",
    "pdf = FPDF()\n",
    "pdf.add_page()\n",
    "pdf.set_font('Times', '', font_size)\n",
    "pdf.set_margins(25.4, 25.4, 25.4)  # One-inch margins\n",
    "\n",
    "max_width = 159  # Width of the available space on the page, considering margins\n",
    "add_wrapped_text(pdf, text, max_width, line_height)\n",
    "\n",
    "pdf.output('output_text.pdf', 'F')\n",
    "\n",
    "print(\"Text extracted and saved as PDF files.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f18db3",
   "metadata": {},
   "source": [
    "USE CSV AGENT - FOR CSV FILES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ffdf704",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.agent_types import AgentType\n",
    "from langchain.agents import create_csv_agent\n",
    "from langchain.llms import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9149e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "# agent = create_csv_agent(\n",
    "#     ChatOpenAI(temperature=0),\n",
    "#     \"2019.csv\",\n",
    "#     verbose=True,\n",
    "#     agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "#     agent_executor_kwargs={\"handle_parsing_errors\": True},\n",
    "    \n",
    "# )\n",
    "\n",
    "# Loop through the list of CSV files and have the agent read each one\n",
    "for csv_file in csv_files:\n",
    "    agent = create_csv_agent(\n",
    "        ChatOpenAI(temperature=0),\n",
    "        csv_file,\n",
    "        verbose=True,\n",
    "        agent_type=AgentType.OPENAI_FUNCTIONS,\n",
    "        agent_executor_kwargs={\"handle_parsing_errors\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4c368ad9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mTo summarize tables in pandas, you can use various functions and methods. Here are a few commonly used ones:\n",
      "\n",
      "1. `df.describe()`: This function provides summary statistics of numerical columns in the dataframe, such as count, mean, standard deviation, minimum, maximum, and quartiles.\n",
      "\n",
      "2. `df.info()`: This method displays a concise summary of the dataframe, including the number of non-null values and the data types of each column.\n",
      "\n",
      "3. `df.value_counts()`: This method returns the count of unique values in each column of the dataframe.\n",
      "\n",
      "4. `df.groupby()`: This method allows you to group the dataframe by one or more columns and perform aggregation functions on the grouped data, such as sum, mean, count, etc.\n",
      "\n",
      "5. `df.pivot_table()`: This method creates a pivot table from the dataframe, allowing you to summarize and aggregate data based on one or more columns.\n",
      "\n",
      "6. `df.crosstab()`: This function computes a cross-tabulation of two or more factors, providing a frequency table of the variables.\n",
      "\n",
      "These are just a few examples of how you can summarize tables in pandas. Depending on your specific requirements, you may need to use other functions or methods.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "To summarize tables in pandas, you can use various functions and methods. Here are a few commonly used ones:\n",
      "\n",
      "1. `df.describe()`: This function provides summary statistics of numerical columns in the dataframe, such as count, mean, standard deviation, minimum, maximum, and quartiles.\n",
      "\n",
      "2. `df.info()`: This method displays a concise summary of the dataframe, including the number of non-null values and the data types of each column.\n",
      "\n",
      "3. `df.value_counts()`: This method returns the count of unique values in each column of the dataframe.\n",
      "\n",
      "4. `df.groupby()`: This method allows you to group the dataframe by one or more columns and perform aggregation functions on the grouped data, such as sum, mean, count, etc.\n",
      "\n",
      "5. `df.pivot_table()`: This method creates a pivot table from the dataframe, allowing you to summarize and aggregate data based on one or more columns.\n",
      "\n",
      "6. `df.crosstab()`: This function computes a cross-tabulation of two or more factors, providing a frequency table of the variables.\n",
      "\n",
      "These are just a few examples of how you can summarize tables in pandas. Depending on your specific requirements, you may need to use other functions or methods.\n"
     ]
    }
   ],
   "source": [
    "response = agent.run(\"Summarize tables\")\n",
    "print (response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d923e",
   "metadata": {},
   "source": [
    "USE OPENAI EMBEDDING - FOR PDF FILES (FROM TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1dc1f74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in c:\\users\\huan bui\\appdata\\roaming\\python\\python310\\site-packages (0.0.235)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\huan bui\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in c:\\users\\huan bui\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (2.0.19)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: langsmith<0.0.8,>=0.0.7 in c:\\users\\huan bui\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (0.0.7)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\huan bui\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (2.28.1)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\huan bui\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (0.5.9)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from langchain) (1.10.11)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\huan bui\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\huan bui\\appdata\\roaming\\python\\python310\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.9.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\huan bui\\appdata\\roaming\\python\\python310\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\huan bui\\appdata\\roaming\\python\\python310\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from pydantic<2,>=1->langchain) (4.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>=1.4->langchain) (2.0.1)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\users\\huan bui\\appdata\\roaming\\python\\python310\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (21.3)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from packaging>=17.0->marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (3.0.9)\n",
      "Requirement already satisfied: openai in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (0.27.8)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\users\\huan bui\\appdata\\roaming\\python\\python310\\site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: tqdm in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\huan bui\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\huan bui\\appdata\\roaming\\python\\python310\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Requirement already satisfied: PyPDF2 in c:\\users\\huan bui\\appdata\\roaming\\python\\python310\\site-packages (3.0.1)\n",
      "Requirement already satisfied: faiss-cpu in c:\\users\\huan bui\\appdata\\roaming\\python\\python310\\site-packages (1.7.4)\n",
      "Requirement already satisfied: tiktoken in c:\\users\\huan bui\\appdata\\roaming\\python\\python310\\site-packages (0.4.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from tiktoken) (2022.7.9)\n",
      "Requirement already satisfied: requests>=2.26.0 in c:\\users\\huan bui\\appdata\\roaming\\python\\python310\\site-packages (from tiktoken) (2.28.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (1.26.14)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\huan bui\\anaconda3\\lib\\site-packages (from requests>=2.26.0->tiktoken) (3.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install langchain\n",
    "!pip install openai\n",
    "!pip install PyPDF2\n",
    "!pip install faiss-cpu\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "003f2d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1b6acf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# location of the pdf file/files. \n",
    "reader = PdfReader('output_text.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "009c03a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data from the file and put them into a variable called raw_text\n",
    "raw_text = ''\n",
    "for i, page in enumerate(reader.pages):\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        raw_text += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "259ac46a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'HAWatcher: Semantics-Aware Anomaly Detection\\nfor Appified Smart Homes\\nChenglong\\nFu, Temple Universit'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# raw_text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f79db367",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(        \n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 1000,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "923efd45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "105"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0fbd0976",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'appified\\nsmarthome,whetherduetoattacksordevicemalfunctions,\\nmayleadtosevereconsequences.Priorworksthatutilizedata\\nminingtechniquestodetectanomaliessufferfromhighfalse\\nalarmratesandmissingmanyrealanomalies.Ourobserva-\\ntionisthatdatamining-basedapproachesmissalargechunk\\nofinformationaboutautomationprograms(alsocalledsmart\\napps)anddevices.WeproposeHomeAutomationWatcher\\n(HAWatcher),asemantics-awareanomalydetectionsystem\\nforappifiedsmarthomes.HAWatchermodelsasmarthome?s\\nnormalbehaviorsbasedonbotheventlogsandsemantics.\\nFigure1:Examplesofanomaliesinasmarthome.\\nGivenahome,HAWatchergenerateshypotheticalcorrela-\\ntionsaccordingtosemanticinformation,suchasapps,device\\nDespiteadvancesinappifiedsmarthome,therearegrow-\\ntypes,relationsandinstallationlocations,andverifiesthem\\ningconcernsaboutitssafetyandsecurity[41].First,IoTde-\\nwitheventlogs.\\nTheminedcorrelationsarerefinedusing\\nvicesmakeitpossibleforcyber-spaceattackstobeextended\\ncorrelations\\nextracted from the installed smart apps.\\nThe'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3b92fb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download embeddings from OpenAI\n",
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "2662e2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = FAISS.from_texts(texts, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ae4feb8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain.vectorstores.faiss.FAISS at 0x2947e8a1990>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docsearch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4a8e0984",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "079dcf49",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = load_qa_chain(OpenAI(), chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "215d8f21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' The article is about leveraging scenarios where IoT devices have one or more other devices nearby to detect anomalous physical behaviors. It discusses jamming and other techniques to detect these behaviors.'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"What is the article about?\"\n",
    "docs = docsearch.similarity_search(query)\n",
    "chain.run(input_documents=docs, question=query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc003796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
